{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ6RW9oGvXVH",
        "colab_type": "code",
        "outputId": "fbb97072-b07a-4274-a80c-33244466833d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import os\n",
        "import string\n",
        "import io\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFQL_I51wwhE",
        "colab_type": "code",
        "outputId": "fea6be27-423e-4924-b451-a27e7ae37fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "DIR_NAMES = ['train-easy/']\n",
        "FILE_NAMES = ['numbers__place_value.txt']\n",
        "\n",
        "BUFFER_SIZE = 50000\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "dataset_dir = './Dataset'\n",
        "\n",
        "# Based on https://www.tensorflow.org/tutorials/load_data/text\n",
        "# Read in all files which are in FILE_NAMES\n",
        "labeled_data_sets = []\n",
        "\n",
        "for file_name in FILE_NAMES:\n",
        "    for dir_name in DIR_NAMES:\n",
        "        concat_dir = os.path.join(dir_name, file_name)\n",
        "        lines_dataset = io.open(os.path.join(dataset_dir, concat_dir), encoding='UTF-8').read().strip().split('\\n')\n",
        "        labeled_data_sets.append(lines_dataset)\n",
        "        \n",
        "# Concatenate all File Data to one Big File Data\n",
        "all_labeled_data = labeled_data_sets[0]\n",
        "for labeled_dataset in labeled_data_sets[1:]:\n",
        "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
        " \n",
        "x_labels = all_labeled_data[::2]\n",
        "y_labels = all_labeled_data[1::2]\n",
        "\n",
        "MAX_QUESTION_LENGTH = 160\n",
        "MAX_ANSWER_LENGTH = 2\n",
        "\n",
        "QUESTION_CHARS = ( ['', ' '] + list(string.ascii_letters + string.digits + string.punctuation))\n",
        "CHAR_TO_INDEX = {char: index for index, char in enumerate(QUESTION_CHARS)}\n",
        "INDEX_TO_CHAR = {index: char for index, char in enumerate(QUESTION_CHARS)}\n",
        "\n",
        "NUM_INDICES = len(QUESTION_CHARS)\n",
        "\n",
        "x_labels_encoded = [[CHAR_TO_INDEX[z] for z in sentence] for sentence in x_labels]\n",
        "y_labels_encoded = [[CHAR_TO_INDEX[z] for z in sentence] for sentence in y_labels]\n",
        "\n",
        "print(CHAR_TO_INDEX)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'': 0, ' ': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27, 'A': 28, 'B': 29, 'C': 30, 'D': 31, 'E': 32, 'F': 33, 'G': 34, 'H': 35, 'I': 36, 'J': 37, 'K': 38, 'L': 39, 'M': 40, 'N': 41, 'O': 42, 'P': 43, 'Q': 44, 'R': 45, 'S': 46, 'T': 47, 'U': 48, 'V': 49, 'W': 50, 'X': 51, 'Y': 52, 'Z': 53, '0': 54, '1': 55, '2': 56, '3': 57, '4': 58, '5': 59, '6': 60, '7': 61, '8': 62, '9': 63, '!': 64, '\"': 65, '#': 66, '$': 67, '%': 68, '&': 69, \"'\": 70, '(': 71, ')': 72, '*': 73, '+': 74, ',': 75, '-': 76, '.': 77, '/': 78, ':': 79, ';': 80, '<': 81, '=': 82, '>': 83, '?': 84, '@': 85, '[': 86, '\\\\': 87, ']': 88, '^': 89, '_': 90, '`': 91, '{': 92, '|': 93, '}': 94, '~': 95}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwuijynIwzZf",
        "colab_type": "code",
        "outputId": "be8ba75a-8883-438a-aa47-724bf44c1921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(x_labels[0])\n",
        "print(x_labels_encoded[0])\n",
        "print(y_labels[0])\n",
        "print(y_labels_encoded[0])\n",
        "print(x_labels[1])\n",
        "print(x_labels_encoded[1])\n",
        "print(len(y_labels_encoded))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is the hundreds digit of 31253?\n",
            "[50, 9, 2, 21, 1, 10, 20, 1, 21, 9, 6, 1, 9, 22, 15, 5, 19, 6, 5, 20, 1, 5, 10, 8, 10, 21, 1, 16, 7, 1, 57, 55, 56, 59, 57, 84]\n",
            "2\n",
            "[56]\n",
            "What is the units digit of 8196?\n",
            "[50, 9, 2, 21, 1, 10, 20, 1, 21, 9, 6, 1, 22, 15, 10, 21, 20, 1, 5, 10, 8, 10, 21, 1, 16, 7, 1, 62, 55, 63, 60, 84]\n",
            "666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGPbZgOQxA9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Padding the Input Questions and Output Answers to the right length\n",
        "x_labels_padded = tf.keras.preprocessing.sequence.pad_sequences(x_labels_encoded, maxlen=MAX_QUESTION_LENGTH, value = 0, padding='post')\n",
        "y_labels_padded = tf.keras.preprocessing.sequence.pad_sequences(y_labels_encoded, maxlen=MAX_ANSWER_LENGTH, value = 0, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKhUSU9GxDLN",
        "colab_type": "code",
        "outputId": "6b49b230-27d0-4620-b3e8-ef9ccf7705c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(x_labels_padded[0])\n",
        "print(y_labels_padded[0])\n",
        "print(len(x_labels_padded))\n",
        "print(len(y_labels_padded))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[50  9  2 21  1 10 20  1 21  9  6  1  9 22 15  5 19  6  5 20  1  5 10  8\n",
            " 10 21  1 16  7  1 57 55 56 59 57 84  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "[56  0]\n",
            "666666\n",
            "666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8UVeDnUxFT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TAKE_SIZE = 100000\n",
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 128\n",
        "embedding_dim = 256\n",
        "steps_per_epoch = TAKE_SIZE//BATCH_SIZE\n",
        "units = 512\n",
        "\n",
        "vocab_inp_size = NUM_INDICES\n",
        "vocab_tar_size = NUM_INDICES\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_labels_padded, y_labels_padded)).take(TAKE_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYnwCIdOxG0v",
        "colab_type": "code",
        "outputId": "8fe5b4f0-86b7-41e4-c4c7-cc324e24556d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(steps_per_epoch)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYLhGEqzytq0",
        "colab_type": "code",
        "outputId": "f1658cde-b33b-4a95-869f-a9cd828201ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 160]), TensorShape([128, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "husaG7_GxIMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLKtmqtgxJWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjkbE-n-yxLT",
        "colab_type": "code",
        "outputId": "d5706122-9bd6-4d2e-ac25-5245f196f3a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (128, 160, 512)\n",
            "Encoder Hidden state shape: (batch size, units) (128, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgPtzdsLxUGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYdR_udnxV1d",
        "colab_type": "code",
        "outputId": "48c14b47-5053-4035-988c-7bf525346eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (128, 512)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (128, 160, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkVgmNc9xbkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYNmiVwbxe8h",
        "colab_type": "code",
        "outputId": "8c6b1b5b-ba49-48c4-9c84-edde62615b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (128, 96)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zid-y78QxhAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  #mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  #mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  #loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2AfjndQxkOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './numbers__place_value_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM0TK762xlzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([CHAR_TO_INDEX['']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t-1], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t-1], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTeNxK4bxnA7",
        "colab_type": "code",
        "outputId": "b55efa35-844a-4c43-d5c3-5b4da5f07ce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "TRAIN_FROM_CHECKPOINT = 1\n",
        "latest_checkpoint_dir = './numbers__place_value_checkpoints'\n",
        "if(TRAIN_FROM_CHECKPOINT):\n",
        "    # restoring the latest checkpoint in checkpoint_dir\n",
        "    checkpoint.restore(tf.train.latest_checkpoint(latest_checkpoint_dir))\n",
        "\n",
        "\n",
        "EPOCHS = 2\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every epochs\n",
        "\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.0008\n",
            "Epoch 1 Batch 100 Loss 0.0004\n",
            "Epoch 1 Batch 200 Loss 0.0246\n",
            "Epoch 1 Batch 300 Loss 0.0007\n",
            "Epoch 1 Batch 400 Loss 0.0005\n",
            "Epoch 1 Batch 500 Loss 0.0004\n",
            "Epoch 1 Batch 600 Loss 0.0002\n",
            "Epoch 1 Batch 700 Loss 0.0004\n",
            "Epoch 1 Loss 0.0013\n",
            "Time taken for 1 epoch 62.45381736755371 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.0001\n",
            "Epoch 2 Batch 100 Loss 0.0001\n",
            "Epoch 2 Batch 200 Loss 0.0017\n",
            "Epoch 2 Batch 300 Loss 0.0015\n",
            "Epoch 2 Batch 400 Loss 0.0002\n",
            "Epoch 2 Batch 500 Loss 0.0002\n",
            "Epoch 2 Batch 600 Loss 0.0000\n",
            "Epoch 2 Batch 700 Loss 0.0001\n",
            "Epoch 2 Loss 0.0030\n",
            "Time taken for 1 epoch 63.381850242614746 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0FmDoxbxp-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  inputs = [[CHAR_TO_INDEX[z] for z in sentence]]\n",
        "    \n",
        "  print(inputs)\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         value=0,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([CHAR_TO_INDEX['']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += INDEX_TO_CHAR[predicted_id] #+ ' '\n",
        "\n",
        "    if INDEX_TO_CHAR[predicted_id] == '':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eodXGH9rymE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5CCGGAOyoQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  #plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
        "\n",
        "  attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
        "  plot_attention(attention_plot, list(sentence), list(result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFvXnvzRyqPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c65f8a6-a860-4ba2-b887-25d3c4432de3"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "checkpoint_dir = './numbers__place_value_checkpoints'\n",
        "#checkpoint_dir = './arithmetic__add_or_sub_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "max_length_targ = MAX_ANSWER_LENGTH\n",
        "max_length_inp = MAX_QUESTION_LENGTH\n",
        "vocab_inp_size = NUM_INDICES\n",
        "vocab_tar_size = NUM_INDICES\n",
        "\n",
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f32c9bf4198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq3jH9PfyyEb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "ae51c49d-808d-4e47-92c1-cd94d78f66a6"
      },
      "source": [
        "count = 0\n",
        "for indx, x in enumerate(x_labels):\n",
        "  print(x)\n",
        "  print(y[indx])\n",
        "  translate(x)\n",
        "\n",
        "  count = count + 1\n",
        "  if(count == 5):\n",
        "    break"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is the hundreds digit of 31253?\n",
            "tf.Tensor([56  0], shape=(2,), dtype=int32)\n",
            "[[50, 9, 2, 21, 1, 10, 20, 1, 21, 9, 6, 1, 22, 15, 10, 21, 20, 1, 5, 10, 8, 10, 21, 1, 16, 7, 1, 56, 58, 59, 62, 58, 84]]\n",
            "Input: What is the units digit of 24584?\n",
            "Predicted translation: 44\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkwAAABICAYAAAAEaqr8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMm6DAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "What is the units digit of 8196?\n",
            "tf.Tensor([60  0], shape=(2,), dtype=int32)\n",
            "[[50, 9, 2, 21, 1, 10, 20, 1, 21, 9, 6, 1, 22, 15, 10, 21, 20, 1, 5, 10, 8, 10, 21, 1, 16, 7, 1, 56, 58, 59, 62, 58, 84]]\n",
            "Input: What is the units digit of 24584?\n",
            "Predicted translation: 44\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkwAAABICAYAAAAEaqr8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMm6DAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "What is the ten thousands digit of 37118?\n",
            "tf.Tensor([57  0], shape=(2,), dtype=int32)\n",
            "[[50, 9, 2, 21, 1, 10, 20, 1, 21, 9, 6, 1, 22, 15, 10, 21, 20, 1, 5, 10, 8, 10, 21, 1, 16, 7, 1, 56, 58, 59, 62, 58, 84]]\n",
            "Input: What is the units digit of 24584?\n",
            "Predicted translation: 44\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkwAAABICAYAAAAEaqr8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAMyUlEQVR4nO3de6wU5RnH8e/DUUEOAQveqCJ4QRAl\n1rReCHoUSWtMrEkvUSvYVloIWmtLtViiRqtWo2LTGm0rKaCmRNtivWutrWjjtWJjjlyqeKNaKVWu\nCsj16R/vHFmX3TOzZ2Z25ujvk2xYZnaeeebd2dln33lnjrk7IiIiIlJfj6ITEBERESk7FUwiIiIi\nMVQwiYiIiMRQwSQiIiISQwWTiIiISAwVTCIiIiIxVDCJiIiIxGhKwWRmY8xsl2asS0RERCRr1owb\nV5rZNuBD4BlgHvAY8Jy7b8195SIiIiIpNatgOhA4ETghegwE1gFPEoqnecALrtuOi4iISAk1pWDa\nYaVmw4ExwPGEAmoPYI279296MiIiIiIxdipipe7+LzNbCawE1gBnAH2KyEVEREQkTtN6mMxsAKE3\naQzh9NwBwAvAE9HjSXdf15RkREREPgXMbGd331x0HmmVYTuaNYapHRgKzGd7gfSUu6/PfeX1c9oJ\nOArYD/jYFXzufnshSUkpmNnJwPcIRf1J7v6WmX0XeMPd/1ZsdtJsZjYr6WvdfUKeuUCn+Tjh4ppX\ngd+7+zt551IWZrYX4TM7gtAOi4BfufvyQhMrATPbBBzu7ouLziWNMmxHs07JHQSsAt4AXgdeS1ss\nmdnpwFhgT6puj+Dup8YsOxy4H9gfMGAroS02AxuBT1XBZGYjgK3u/nL0/y8C3wIWAtd1t6sZU+4b\n44DfAL+NYuwczWoBpgJNK5jM7L7O5sdtS0WcLrdHWRS8j+5R9f82YBvwUvT/wwjt+vccc6jO57go\nhwUVORih1/6rwBVmdpy7v9iknApjZqOBPwPLCVdiA4wDppjZSe7+TN2FS8TMdgNGE74rn6m8CMrM\nWoEL3P2KTpa/sc6sFuDiaBgM7n5+Azl9hvA5GwosA25z97dilukBTAOOBh5091vM7OxoWg/gT8Al\n7r6pWduRlWbduLIfcBqwBBgPLDSzpWZ2u5lNMLMDGglmZtcDvwOGAKuBFVWPOL8gHFj6AeuBQ4Av\nAC8CX0uYw35mZjWmm5ntlyRGicwCjgAws0HAvUB/wi+2q5qRQFbtmcG+MRWY6O5TgC0V058FPpc0\nj4xU576WUOS3Ae8lCZBBe6SW0Xtb2D7q7l/ueABPA48A+7p7m7u3AYMIX9jP5ZlHhaeAh6ty2Bd4\nCPgLMBh4ELghzyRKdAycDtwBHOzuZ7n7WcDBwJ100gZm1hadaSicmR0KLCbs108Cz5vZ4IqX9AEu\niwlzHqGQHln1MEJ7jCQU1p3l8U40fAYz25/QUzeN8B15LvBS1OHQmZ8CFxDGKF9sZlcQ3qPbgJnA\nt4FL89yOiu05xcx+aGZ7J3l9LHdv+gPoSRjPdDnh9NwG4M0Gll8OfD3F+lcAh0XP1wDDoufHA+0J\nY2wF9qwxfQDhl3AhbdvF9lhNONgATAHmRc/HNPK+pMwhk/bMYN9YDwyOnr8PHBA9PxDYUPR7FeVy\nA3BZM9qjLO9tGfbRaH3LgBE1ph8K/LeJORxSY/oIYFn0/AhgRdnf14zy2NBxDK+aPryzz2xl/oQz\nHwOalXONXO4j9Ly0Em678wfgbWBoNH+vuDYlFDavA8dXTd9ca5+tE2NbRZvcQbjlT2v0/17AA8Af\nY2K8DpxSsU9uBcZVzP8K8Gqe2xG9/ifRMv+JjoMj075PRf1plG0VDydUjoMaWL4HoTeoq4zwxQjw\nLrBP9PxtwunDpDFqDQDrQxhH0J20AB3do2MJv1QBXiN8UJshq/ZMu2+8Q/gVU62N0B5lcAuhZyWJ\ntO2RhSze2zLsoxBy/myN6QOB3k3MYWCN6Xuz/WrjteQ/5KIsx8A1hJ7XavsTCu16VlUsN4Ri/1TY\nMcCl7r7O3Ze5+2mEoulxM6t1PNqBu18DnAnMNLOro1NjaRwNXOnRxVju/iFwZZRrZwYC7dEyiwgF\nU+Ux6J/U3n+JlslqO84FvuPu+wC/BB41sy9FPaM7mdnARntCm9IdWTHAekz0GAXsCiwlVLAzo3+T\nmkE4tXd5F1NaABxOqGL/AVxkZluBiYQBk3VVnF914BozqxyL1ULYzqK/oBq1ADjHzB4gfBlNi6bv\nQ8JTP12VQ3um3TdmADdGg7wBBpnZccB1KWJmbVgDr03bHl2W8Xtb2D5a5S5gtpn9mHCaFsIXyLWE\nHoJmuJvwZTIVeD6adiRhH+3I4SjglTxWXsJj4J1sb4+no2mjCe/JHZ0sdxfwhJktI2zL/Oh7YAfu\n3tCwkS7oSVXx6e4/ik55Pk4oIGK5+7Nm9nnC5/45M0u0XHWYipz+VzVvOTuO6au2jHDK7N9mNoyw\nT4wgjDeE0BtbHffjCWSzHf2JxhW6e0fh9XA070hgDuHHcUvSgM06f7uaUCAtIxRG3wcec/c3kwao\nGgjWAxgXDfxsJ3S7fcTjB4P9jND1CXAJ4Xz/PMKB97SYZUd2pEQ4r1s5cG0ToXqeHhMjlWgw8Hh3\nX5vRwOCLgHuACwmD+joGs55KKCjzlLo9s9w33P06M+sHPErogp5HuBBgurvfHLs1Gaox+NEIv8xO\nJozpSWI34MxG2yOjfSzLz0qR+2ilcwinRG9l+wUBWwg/+i6st1DGn9nJwM8JY9M6juFbCPtERw6L\nCT8A88ij8GNglalRLrPY3h6bgV8TTsvUM5lwKmwooT1nE07DF+FlwjjaRZUT3X1K9EV/b9JA7r4G\nON3MJhLGuzXaQ/OEmW0hjPEdzvYLCyBcVR73A2UOcLuZ3U/oILkGmG7hSsZthPdkbhO24xVCofZm\nFO8qM5tJOIYuBr5Jg73CzbqtwCTCmIMlKWIk7YFydz+xC/H7A6s8YYOY2WzgB+6+ttF1pRWt+3x3\nfz96Xpe7n50wZgvQ191XVUwbAqx3905/DWQhTXvmsW+YWW/Ch60HsMjdP2g0r7RqbNc2winkx4BZ\n7r5lx6ViY1Sq2x5Z7mNZfVaK3kercmkljGuDcNVvp/eQy+kz21AOWedR5DGwlugzW9keia/ErmyX\nXJKLX/80oM3dT64z/2bgHHdvqGiITueNAu5O8j6ZWfXA8mfd/ZGK+dcTLjb4RicxehCKolGE+yte\na2ZnEHpAexOuUD8vyf7a1e2IljkPGOPuiS7kShSzGQWTiIiISHdW5CA3ERERkW6h8IIpOl1X2PKK\noRjdIUYZclAMxcg7RhlyUAzFqKfwgglI2wCpG1AxFKMbxChDDoqhGHnHKEMOiqEYNZWhYBIREREp\ntVwHfe9iPb3XR1fv17aZjexMzy6vI+3yiqEY3SFGGXJQDMXIO0YZclCMxmNYS/ytjDb5BnaxXevO\nH3LomtgYK1ZsY8CA+v08b7T3qTuvQ9y2vM+q99y95r2mcr0PUy9aOdrG5rkKERERKVBL336pY9z6\n0EPxL4px1qDRqWP81ecurTdPp+REREREYqhgEhEREYmhgklEREQkRsMFk5lNMzM3s5vySEhERESk\nbBr9uzTHEO5h0J5POiIiIiLlk7hgiv6C+xxgArAq5uUiIiIinxiN9DDNAOa6e9K/DC8iIiLyiZDo\nPkxmNhE4CBif4LWTiG493oveqZITERERKYPYgsnMhgFXA8e6++a417v7DEJvFH2tf363ERcRERFp\nkiQ9TKOA3YGFZtYxrQVoM7PJQKu7b8wpPxEREZHCJSmY7gHmV02bDSwh9DxtyjopERERkTKJLZjc\nfTWwunKama0DVrr7grwSExERESkL3elbREREJEaiq+SqufsJGechIiIiUlrqYRIRERGJ0aUeJhER\nERGArWs/SB1jz5bWDDLJl3qYRERERGKoYBIRERGJoYJJREREJIYKJhEREZEYDRdMZjbNzNzMbsoj\nIREREZGyaahgMrNjgElAez7piIiIiJRP4oLJzPoBc4AJwKrcMhIREREpmUZ6mGYAc919Xl7JiIiI\niJRRohtXmtlE4CBgfILXTiKctqMXvVMlJyIiIlIGsQWTmQ0DrgaOdffNca939xmE3ij6Wn9PnaGI\niIhIwZL0MI0CdgcWmlnHtBagzcwmA63uvjGn/EREREQKl6RgugeYXzVtNrCE0PO0KeukRERERMok\ntmBy99XA6sppZrYOWOnuC/JKTERERKQsdKdvERERkRiJrpKr5u4nZJyHiIiISGmph0lEREQkhrnn\nd+W/mb0LLI152e7AeylWk3Z5xVCM7hCjDDkohmLkHaMMOSjGpzvGYHffo+Ycdy/0AcwvcnnFUIzu\nEKMMOSiGYuQdoww5KIZi1HvolJyIiIhIDBVMIiIiIjHKUDDNKHh5xVCM7hCjDDkohmLkHaMMOSiG\nYtSU66BvERERkU+CMvQwiYiIiJSaCiYRERGRGCqYRERERGKoYBIRERGJoYJJREREJMb/AQ1IuKE3\nMm6DAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "What is the hundreds digit of 1229?\n",
            "tf.Tensor([56  0], shape=(2,), dtype=int32)\n",
            "[[50, 9, 2, 21, 1, 10, 20, 1, 21, 9, 6, 1, 22, 15, 10, 21, 20, 1, 5, 10, 8, 10, 21, 1, 16, 7, 1, 56, 58, 59, 62, 58, 84]]\n",
            "Input: What is the units digit of 24584?\n",
            "Predicted translation: 44\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkwAAABICAYAAAAEaqr8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMm6DAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "What is the units digit of 834?\n",
            "tf.Tensor([58  0], shape=(2,), dtype=int32)\n",
            "[[50, 9, 2, 21, 1, 10, 20, 1, 21, 9, 6, 1, 22, 15, 10, 21, 20, 1, 5, 10, 8, 10, 21, 1, 16, 7, 1, 56, 58, 59, 62, 58, 84]]\n",
            "Input: What is the units digit of 24584?\n",
            "Predicted translation: 44\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkwAAABICAYAAAAEaqr8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAMyUlEQVR4nO3de6wU5RnH8e/DUUEOAQveqCJ4QRAl\n1rReCHoUSWtMrEkvUSvYVloIWmtLtViiRqtWo2LTGm0rKaCmRNtivWutrWjjtWJjjlyqeKNaKVWu\nCsj16R/vHFmX3TOzZ2Z25ujvk2xYZnaeeebd2dln33lnjrk7IiIiIlJfj6ITEBERESk7FUwiIiIi\nMVQwiYiIiMRQwSQiIiISQwWTiIiISAwVTCIiIiIxVDCJiIiIxGhKwWRmY8xsl2asS0RERCRr1owb\nV5rZNuBD4BlgHvAY8Jy7b8195SIiIiIpNatgOhA4ETghegwE1gFPEoqnecALrtuOi4iISAk1pWDa\nYaVmw4ExwPGEAmoPYI279296MiIiIiIxdipipe7+LzNbCawE1gBnAH2KyEVEREQkTtN6mMxsAKE3\naQzh9NwBwAvAE9HjSXdf15RkREREPgXMbGd331x0HmmVYTuaNYapHRgKzGd7gfSUu6/PfeX1c9oJ\nOArYD/jYFXzufnshSUkpmNnJwPcIRf1J7v6WmX0XeMPd/1ZsdtJsZjYr6WvdfUKeuUCn+Tjh4ppX\ngd+7+zt551IWZrYX4TM7gtAOi4BfufvyQhMrATPbBBzu7ouLziWNMmxHs07JHQSsAt4AXgdeS1ss\nmdnpwFhgT6puj+Dup8YsOxy4H9gfMGAroS02AxuBT1XBZGYjgK3u/nL0/y8C3wIWAtd1t6sZU+4b\n44DfAL+NYuwczWoBpgJNK5jM7L7O5sdtS0WcLrdHWRS8j+5R9f82YBvwUvT/wwjt+vccc6jO57go\nhwUVORih1/6rwBVmdpy7v9iknApjZqOBPwPLCVdiA4wDppjZSe7+TN2FS8TMdgNGE74rn6m8CMrM\nWoEL3P2KTpa/sc6sFuDiaBgM7n5+Azl9hvA5GwosA25z97dilukBTAOOBh5091vM7OxoWg/gT8Al\n7r6pWduRlWbduLIfcBqwBBgPLDSzpWZ2u5lNMLMDGglmZtcDvwOGAKuBFVWPOL8gHFj6AeuBQ4Av\nAC8CX0uYw35mZjWmm5ntlyRGicwCjgAws0HAvUB/wi+2q5qRQFbtmcG+MRWY6O5TgC0V058FPpc0\nj4xU576WUOS3Ae8lCZBBe6SW0Xtb2D7q7l/ueABPA48A+7p7m7u3AYMIX9jP5ZlHhaeAh6ty2Bd4\nCPgLMBh4ELghzyRKdAycDtwBHOzuZ7n7WcDBwJ100gZm1hadaSicmR0KLCbs108Cz5vZ4IqX9AEu\niwlzHqGQHln1MEJ7jCQU1p3l8U40fAYz25/QUzeN8B15LvBS1OHQmZ8CFxDGKF9sZlcQ3qPbgJnA\nt4FL89yOiu05xcx+aGZ7J3l9LHdv+gPoSRjPdDnh9NwG4M0Gll8OfD3F+lcAh0XP1wDDoufHA+0J\nY2wF9qwxfQDhl3AhbdvF9lhNONgATAHmRc/HNPK+pMwhk/bMYN9YDwyOnr8PHBA9PxDYUPR7FeVy\nA3BZM9qjLO9tGfbRaH3LgBE1ph8K/LeJORxSY/oIYFn0/AhgRdnf14zy2NBxDK+aPryzz2xl/oQz\nHwOalXONXO4j9Ly0Em678wfgbWBoNH+vuDYlFDavA8dXTd9ca5+tE2NbRZvcQbjlT2v0/17AA8Af\nY2K8DpxSsU9uBcZVzP8K8Gqe2xG9/ifRMv+JjoMj075PRf1plG0VDydUjoMaWL4HoTeoq4zwxQjw\nLrBP9PxtwunDpDFqDQDrQxhH0J20AB3do2MJv1QBXiN8UJshq/ZMu2+8Q/gVU62N0B5lcAuhZyWJ\ntO2RhSze2zLsoxBy/myN6QOB3k3MYWCN6Xuz/WrjteQ/5KIsx8A1hJ7XavsTCu16VlUsN4Ri/1TY\nMcCl7r7O3Ze5+2mEoulxM6t1PNqBu18DnAnMNLOro1NjaRwNXOnRxVju/iFwZZRrZwYC7dEyiwgF\nU+Ux6J/U3n+JlslqO84FvuPu+wC/BB41sy9FPaM7mdnARntCm9IdWTHAekz0GAXsCiwlVLAzo3+T\nmkE4tXd5F1NaABxOqGL/AVxkZluBiYQBk3VVnF914BozqxyL1ULYzqK/oBq1ADjHzB4gfBlNi6bv\nQ8JTP12VQ3um3TdmADdGg7wBBpnZccB1KWJmbVgDr03bHl2W8Xtb2D5a5S5gtpn9mHCaFsIXyLWE\nHoJmuJvwZTIVeD6adiRhH+3I4SjglTxWXsJj4J1sb4+no2mjCe/JHZ0sdxfwhJktI2zL/Oh7YAfu\n3tCwkS7oSVXx6e4/ik55Pk4oIGK5+7Nm9nnC5/45M0u0XHWYipz+VzVvOTuO6au2jHDK7N9mNoyw\nT4wgjDeE0BtbHffjCWSzHf2JxhW6e0fh9XA070hgDuHHcUvSgM06f7uaUCAtIxRG3wcec/c3kwao\nGgjWAxgXDfxsJ3S7fcTjB4P9jND1CXAJ4Xz/PMKB97SYZUd2pEQ4r1s5cG0ToXqeHhMjlWgw8Hh3\nX5vRwOCLgHuACwmD+joGs55KKCjzlLo9s9w33P06M+sHPErogp5HuBBgurvfHLs1Gaox+NEIv8xO\nJozpSWI34MxG2yOjfSzLz0qR+2ilcwinRG9l+wUBWwg/+i6st1DGn9nJwM8JY9M6juFbCPtERw6L\nCT8A88ij8GNglalRLrPY3h6bgV8TTsvUM5lwKmwooT1nE07DF+FlwjjaRZUT3X1K9EV/b9JA7r4G\nON3MJhLGuzXaQ/OEmW0hjPEdzvYLCyBcVR73A2UOcLuZ3U/oILkGmG7hSsZthPdkbhO24xVCofZm\nFO8qM5tJOIYuBr5Jg73CzbqtwCTCmIMlKWIk7YFydz+xC/H7A6s8YYOY2WzgB+6+ttF1pRWt+3x3\nfz96Xpe7n50wZgvQ191XVUwbAqx3905/DWQhTXvmsW+YWW/Ch60HsMjdP2g0r7RqbNc2winkx4BZ\n7r5lx6ViY1Sq2x5Z7mNZfVaK3kercmkljGuDcNVvp/eQy+kz21AOWedR5DGwlugzW9keia/ErmyX\nXJKLX/80oM3dT64z/2bgHHdvqGiITueNAu5O8j6ZWfXA8mfd/ZGK+dcTLjb4RicxehCKolGE+yte\na2ZnEHpAexOuUD8vyf7a1e2IljkPGOPuiS7kShSzGQWTiIiISHdW5CA3ERERkW6h8IIpOl1X2PKK\noRjdIUYZclAMxcg7RhlyUAzFqKfwgglI2wCpG1AxFKMbxChDDoqhGHnHKEMOiqEYNZWhYBIREREp\ntVwHfe9iPb3XR1fv17aZjexMzy6vI+3yiqEY3SFGGXJQDMXIO0YZclCMxmNYS/ytjDb5BnaxXevO\nH3LomtgYK1ZsY8CA+v08b7T3qTuvQ9y2vM+q99y95r2mcr0PUy9aOdrG5rkKERERKVBL336pY9z6\n0EPxL4px1qDRqWP81ecurTdPp+REREREYqhgEhEREYmhgklEREQkRsMFk5lNMzM3s5vySEhERESk\nbBr9uzTHEO5h0J5POiIiIiLlk7hgiv6C+xxgArAq5uUiIiIinxiN9DDNAOa6e9K/DC8iIiLyiZDo\nPkxmNhE4CBif4LWTiG493oveqZITERERKYPYgsnMhgFXA8e6++a417v7DEJvFH2tf363ERcRERFp\nkiQ9TKOA3YGFZtYxrQVoM7PJQKu7b8wpPxEREZHCJSmY7gHmV02bDSwh9DxtyjopERERkTKJLZjc\nfTWwunKama0DVrr7grwSExERESkL3elbREREJEaiq+SqufsJGechIiIiUlrqYRIRERGJ0aUeJhER\nERGArWs/SB1jz5bWDDLJl3qYRERERGKoYBIRERGJoYJJREREJIYKJhEREZEYDRdMZjbNzNzMbsoj\nIREREZGyaahgMrNjgElAez7piIiIiJRP4oLJzPoBc4AJwKrcMhIREREpmUZ6mGYAc919Xl7JiIiI\niJRRohtXmtlE4CBgfILXTiKctqMXvVMlJyIiIlIGsQWTmQ0DrgaOdffNca939xmE3ij6Wn9PnaGI\niIhIwZL0MI0CdgcWmlnHtBagzcwmA63uvjGn/EREREQKl6RgugeYXzVtNrCE0PO0KeukRERERMok\ntmBy99XA6sppZrYOWOnuC/JKTERERKQsdKdvERERkRiJrpKr5u4nZJyHiIiISGmph0lEREQkhrnn\nd+W/mb0LLI152e7AeylWk3Z5xVCM7hCjDDkohmLkHaMMOSjGpzvGYHffo+Ycdy/0AcwvcnnFUIzu\nEKMMOSiGYuQdoww5KIZi1HvolJyIiIhIDBVMIiIiIjHKUDDNKHh5xVCM7hCjDDkohmLkHaMMOSiG\nYtSU66BvERERkU+CMvQwiYiIiJSaCiYRERGRGCqYRERERGKoYBIRERGJoYJJREREJMb/AQ1IuKE3\nMm6DAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIDXZcmny1Io",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "a142be3a-447c-4405-e298-9fc65434d509"
      },
      "source": [
        "from google.colab import files\n",
        "!zip -r numbers__place_value_checkpoints.zip ./numbers__place_value_checkpoints"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: numbers__place_value_checkpoints/ (stored 0%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-1.index (deflated 70%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-3.index (deflated 70%)\n",
            "updating: numbers__place_value_checkpoints/checkpoint (deflated 38%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-2.index (deflated 70%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-3.data-00000-of-00002 (deflated 60%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-2.data-00001-of-00002 (deflated 20%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-3.data-00001-of-00002 (deflated 20%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-4.data-00001-of-00002 (deflated 20%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-2.data-00000-of-00002 (deflated 60%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-1.data-00000-of-00002 (deflated 60%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-4.index (deflated 70%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-1.data-00001-of-00002 (deflated 20%)\n",
            "updating: numbers__place_value_checkpoints/ckpt-4.data-00000-of-00002 (deflated 60%)\n",
            "updating: numbers__place_value_checkpoints/.ipynb_checkpoints/ (stored 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}